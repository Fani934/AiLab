{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a700d9c-f235-4fc8-b893-48e57b0f9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from PIL import Image\n",
    "import pandas as pd  # For reading Excel file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset Class\n",
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, tokenizer):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and preprocess image\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        image_tensor = transform(image)\n",
    "\n",
    "        # Tokenize text\n",
    "        text = self.texts[idx]\n",
    "        text_tokens = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512\n",
    "        )\n",
    "        text_tokens = {key: value.squeeze(0) for key, value in text_tokens.items()}\n",
    "\n",
    "        # Label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return image_tensor, text_tokens, label\n",
    "\n",
    "\n",
    "# Model Definition\n",
    "class ImageTextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTextClassifier, self).__init__()\n",
    "\n",
    "        # CNN for image analysis (using ResNet-101)\n",
    "        self.image_model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.fc = nn.Linear(self.image_model.fc.in_features, 512)\n",
    "\n",
    "        # DistilBERT for text analysis\n",
    "        self.text_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.text_fc = nn.Linear(768, 512)\n",
    "\n",
    "        # Classifier combining both modalities\n",
    "        self.classifier = nn.Linear(512 + 512, 2)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Image features\n",
    "        image_features = self.image_model(image)\n",
    "\n",
    "        # Text features\n",
    "        text_features = self.text_model(**text).last_hidden_state[:, 0, :]\n",
    "        text_features = self.text_fc(text_features)\n",
    "\n",
    "        # Combine features\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        output = self.classifier(combined_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Train Function\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, texts, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            texts = {key: value.to(device) for key, value in texts.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, texts, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                texts = {key: value.to(device) for key, value in texts.items()}\n",
    "\n",
    "                outputs = model(images, texts)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        # Save model checkpoint\n",
    "        torch.save(model.state_dict(), \"image_text_classifier.pth\")\n",
    "\n",
    "\n",
    "def prepare_dataset(excel_file, images_folder):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_file)\n",
    "\n",
    "    # List all image files in the images folder\n",
    "    image_files = os.listdir(images_folder)\n",
    "    image_files = [os.path.join(images_folder, img) for img in image_files]\n",
    "\n",
    "    # Extract 'Text' and 'Label' columns\n",
    "    texts = df['Text'].tolist()\n",
    "    labels = df['Label'].tolist()\n",
    "\n",
    "    # Convert string labels to integer labels\n",
    "    label_map = {label: idx for idx, label in enumerate(set(labels))}\n",
    "    labels = [label_map[label] for label in labels]\n",
    "\n",
    "    # Train-Test Split\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_images, val_images = train_test_split(image_files, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = ImageTextDataset(train_images, train_texts, train_labels, tokenizer)\n",
    "    val_dataset = ImageTextDataset(val_images, val_texts, val_labels, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths for your dataset\n",
    "    excel_file = r\"C:\\Users\\Lenovo\\Desktop\\model train\\balanced_text_classification_dataset.xlsx\"  # Path to your custom Excel file\n",
    "    images_folder = r\"C:\\Users\\Lenovo\\Downloads\\archive\\natural_images\\real images\"  # Path to folder containing images\n",
    "\n",
    "    # Prepare data\n",
    "    train_loader, val_loader = prepare_dataset(excel_file, images_folder)\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = ImageTextClassifier()\n",
    "    train_model(model, train_loader, val_loader, epochs=5)\n",
    "\n",
    "    print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2436d5-5ff0-420a-9a4b-65b982c0d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "  Student  Math  Science  English\n",
      "0       A  85.0     88.0      NaN\n",
      "1       B  90.0      NaN     85.0\n",
      "2       C   NaN     90.0     78.0\n",
      "3       D  95.0     85.0     88.0\n",
      "4       E  92.0     91.0     90.0\n",
      "\n",
      "DataFrame after forward fill:\n",
      "  Student  Math  Science  English\n",
      "0       A  85.0     88.0      NaN\n",
      "1       B  90.0     88.0     85.0\n",
      "2       C  90.0     90.0     78.0\n",
      "3       D  95.0     85.0     88.0\n",
      "4       E  92.0     91.0     90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_13452\\3144338343.py:19: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data for student scores (with some missing values)\n",
    "data = {\n",
    "    'Student': ['A', 'B', 'C', 'D', 'E'],\n",
    "    'Math': [85, 90, None, 95, 92],\n",
    "    'Science': [88, None, 90, 85, 91],\n",
    "    'English': [None, 85, 78, 88, 90]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Forward fill to interpolate missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the DataFrame after forward fill\n",
    "print(\"\\nDataFrame after forward fill:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d870e8-61c8-4d0b-b4d5-a8fbc4909ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 66.67%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [1 1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fail       0.50      1.00      0.67         1\n",
      "        Pass       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.75      0.75      0.67         3\n",
      "weighted avg       0.83      0.67      0.67         3\n",
      "\n",
      "\n",
      "Test Predictions:\n",
      "Test Sample 1: Predicted - Pass, Actual - Pass\n",
      "Test Sample 2: Predicted - Fail, Actual - Fail\n",
      "Test Sample 3: Predicted - Fail, Actual - Pass\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample student academic data (features: marks in Math, Science, and English)\n",
    "data = {\n",
    "    'Math': [85, 90, 75, 95, 88, 72, 92, 80],\n",
    "    'Science': [88, 85, 92, 95, 89, 78, 91, 82],\n",
    "    'English': [78, 85, 70, 88, 90, 74, 92, 80],\n",
    "    'Performance': ['Pass', 'Pass', 'Fail', 'Pass', 'Pass', 'Fail', 'Pass', 'Fail']  # Target variable\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[['Math', 'Science', 'English']]  # Features\n",
    "y = df['Performance']  # Target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the output\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Display test predictions\n",
    "print(\"\\nTest Predictions:\")\n",
    "for idx, prediction in enumerate(y_pred):\n",
    "    print(f\"Test Sample {idx+1}: Predicted - {prediction}, Actual - {y_test.values[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b65a7d5-c62d-427b-9afe-660872ea6e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.33%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [2 0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.33      1.00      0.50         1\n",
      "         Low       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Sample dataset for loan risk classification\n",
    "data_loan = {\n",
    "    'Income': [4000, 5000, 3000, 8000, 7000, 2000, 6000, 4500],\n",
    "    'Credit_Score': [700, 750, 650, 800, 720, 580, 690, 710],\n",
    "    'Loan Amount': [2000, 2500, 1500, 5000, 3000, 1000, 4000, 2700],\n",
    "    'Risk': ['Low', 'Low', 'High', 'Low', 'Low', 'High', 'Low', 'High']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_loan = pd.DataFrame(data_loan)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_loan[['Income', 'Credit_Score', 'Loan Amount']]\n",
    "y = df_loan['Risk']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model_loan = LogisticRegression(random_state=42)\n",
    "model_loan.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_loan = model_loan.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_loan) * 100:.2f}%\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_loan))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_loan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90de5fe5-3419-4881-84b8-b5569ad27837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.33%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [2 0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Not Spam       0.33      1.00      0.50         1\n",
      "        Spam       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Sample email dataset (features: word frequency, email length)\n",
    "data_spam = {\n",
    "    'Email Length': [100, 50, 300, 20, 150, 250, 90, 400],\n",
    "    'Spam Keywords': [3, 1, 5, 0, 2, 6, 1, 7],\n",
    "    'Spam': ['Not Spam', 'Spam', 'Spam', 'Not Spam', 'Not Spam', 'Spam', 'Not Spam', 'Spam']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_spam = pd.DataFrame(data_spam)\n",
    "\n",
    "# Split dataset\n",
    "X_spam = df_spam[['Email Length', 'Spam Keywords']]\n",
    "y_spam = df_spam['Spam']\n",
    "\n",
    "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(X_spam, y_spam, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "model_spam = MultinomialNB()\n",
    "model_spam.fit(X_train_spam, y_train_spam)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_spam = model_spam.predict(X_test_spam)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test_spam, y_pred_spam) * 100:.2f}%\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_spam, y_pred_spam))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_spam, y_pred_spam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155add40-7b35-4bbe-9cf5-4f1247bc292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Disease A       1.00      1.00      1.00         1\n",
      "   Disease B       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Sample patient symptom data\n",
    "data = {\n",
    "    'Fever': [1, 0, 1, 0, 1, 0],\n",
    "    'Cough': [1, 0, 1, 1, 0, 1],\n",
    "    'Fatigue': [0, 1, 1, 0, 1, 1],\n",
    "    'Disease': ['Disease A', 'Disease B', 'Disease A', 'Disease B', 'Disease A', 'Disease B']  # Target variable\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[['Fever', 'Cough', 'Fatigue']]  # Features\n",
    "y = df['Disease']  # Target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74abd013-2145-4e61-ad2a-5e2d4a7282d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coefficients: [  202.65957447 11542.55319149]\n",
      "Intercept: -52074.46808510646\n",
      "\n",
      "Predicted Prices: [399414.89361702 295265.95744681]\n",
      "\n",
      "Actual Prices: [400000 250000]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample house dataset\n",
    "data_houses = {\n",
    "    'Square Footage': [1500, 2000, 1200, 2500, 1800, 1600, 2200, 2100],\n",
    "    'Num Bedrooms': [3, 4, 2, 5, 3, 2, 4, 4],\n",
    "    'Price': [300000, 400000, 200000, 500000, 350000, 250000, 450000, 420000]\n",
    "}\n",
    "\n",
    "df_houses = pd.DataFrame(data_houses)\n",
    "\n",
    "X_houses = df_houses[['Square Footage', 'Num Bedrooms']]\n",
    "y_houses = df_houses['Price']\n",
    "\n",
    "X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(X_houses, y_houses, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model_house = LinearRegression()\n",
    "model_house.fit(X_train_house, y_train_house)\n",
    "\n",
    "# Predict house prices\n",
    "y_pred_house = model_house.predict(X_test_house)\n",
    "\n",
    "print(\"\\nCoefficients:\", model_house.coef_)\n",
    "print(\"Intercept:\", model_house.intercept_)\n",
    "print(\"\\nPredicted Prices:\", y_pred_house)\n",
    "print(\"\\nActual Prices:\", y_test_house.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e5c8b-22a0-443f-ad18-2ece370c0af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
